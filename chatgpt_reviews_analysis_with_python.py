# -*- coding: utf-8 -*-
"""ChatGPT Reviews Analysis with Python.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zY4trDf4CROpFT_SpKZUdQF7bGfpyVEE

# Importing Required Libraries
"""

import pandas as pd
import plotly.graph_objects as go
from sklearn.feature_extraction.text import CountVectorizer
from collections import Counter
import plotly.express as px
import plotly.io as pio
pio.templates.default = "plotly_white"

from google.colab import files
upload = files.upload()

""" Loading Dataset"""

df = pd.read_csv("chatgpt_reviews.csv")
df.head()

# check the dimension of the dataset
num_row, num_col = df.shape
print('The dataset contains {} rows and {} columns.'. format(num_row, num_col))

# check the attributes in the dataset
print('Attributes in the dataset: ', df.columns.values)

# Inspecting the Dataset's Details
df.info()

""" Cleaning the Dataset"""

df.isnull().sum()

df['Review'] = df['Review'].astype(str).fillna('')

# Importing TextBlob for Sentiment Analysis
from textblob import TextBlob

# Sentiment Analysis Function
def get_sentiment(review):
    sentiment = TextBlob(review).sentiment.polarity
    if sentiment > 0:
        return 'Positive'
    elif sentiment < 0:
        return 'Negative'
    else:
        return 'Neutral'

# apply sentiment analysis
df['Sentiment'] = df['Review'].apply(get_sentiment)

sentiment_distribution = df['Sentiment'].value_counts()

!pip install pywaffle

"""# Proportion of Positive & Negative Reviews"""

# distribution of positive and negative review
from pywaffle import Waffle
import matplotlib.pyplot as plt
import numpy as np

# Assuming 'Sentiment' column contains 'Positive' and 'Negative' values
# compute the proportion of the positive and negative review
category = df['Sentiment'].value_counts()  # Changed 'category' to 'Sentiment'
proportion = np.round((category.values / df.shape[0]) * 100, 2)

# visualize the proportion in waffle plot
# Added a third color to 'colors' to match the number of sentiments
fig = plt.figure(
    FigureClass=Waffle,
    rows=5,
    values=proportion,
    colors=['rebeccapurple', 'plum', 'lightgray'],  # Added 'lightgray' for Neutral
    icons=['thumbs-up', 'thumbs-down', 'meh'],  # Added 'meh' icon for Neutral
    figsize=(15, 8),
    font_size=40,
    legend={
        'labels': ['Positive', 'Negative', 'Neutral'],  # Added 'Neutral' to labels
        'loc': 'upper left',
        'bbox_to_anchor': (1, 1)
    }
)

# set the title of the plot
plt.title('Proportion of Positive & Negative Reviews', fontsize=20)

# display the plot
plt.show()

# define a function take in a dataframe,
# column name (text) in the dataframe and list of specified stopwords to generate wod clouds
from wordcloud import WordCloud, STOPWORDS
import matplotlib.pyplot as plt #Importing matplotlib to use plt.figure

def generate_wordcloud(df, textCol, additional_stopwordList):
    # join the words in the text column to form a single string
    text_combined = ' '.join(df[textCol])

    stopwords = set(STOPWORDS)
    stopwords.update(additional_stopwordList)

    # create a WordCloud object
    #Change the colormap to a valid matplotlib colormap
    wordcloud = WordCloud(width=800, height=400, stopwords=stopwords,
                          background_color='white',colormap='viridis').generate(text_combined)

    # display the WordCloud
    plt.figure(figsize=(15, 10))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis('off')
    plt.show()

generate_wordcloud(df, 'Review', ['ChatGPT', 'app'])

# let's view the vocab in the positive review
positive_df = df[df['Sentiment']=='Positive'] # Changed 'category' to 'Sentiment'

generate_wordcloud(positive_df, 'Review', ['ChatGPT', 'app', 'use', 'user', 'S'])

"""Word Cloud of Negtive Reviews"""

# let's view the vocab in the negative review
negative_df = df[df['Sentiment']=='Negative']

generate_wordcloud(negative_df, 'Review', ['ChatGPT', 'app', 'use', 'user', 'S'])

""" Sentiment Distribution Visualization"""

sentiment_distribution = df['Sentiment'].value_counts()
fig = go.Figure(data=[go.Bar(
    x=sentiment_distribution.index,
    y=sentiment_distribution.values,
    marker_color=['green', 'gray', 'red'],
)])
fig.update_layout(
    title='Sentiment Distribution of ChatGPT Reviews',
    xaxis_title='Sentiment',
    yaxis_title='Number of Reviews',
    width=800,
    height=600
)
fig.show()

"""Extracting Common Phrases in Positive Reviews"""

# filter reviews with positive sentiment
positive_reviews = df[df['Sentiment'] == 'Positive']['Review']

# use CountVectorizer to extract common phrases (n-grams)
vectorizer = CountVectorizer(ngram_range=(2, 3), stop_words='english', max_features=100)
X = vectorizer.fit_transform(positive_reviews)

"""# Plot 1 | Rating of ChatGPT Application"""

import plotly.express as px

Ratings = df.Ratings.value_counts().sort_index()

# create a bar plot using plotly express
fig = px.bar(x=Ratings.index, y=Ratings.values,
             labels={'x':'Rating','y':'Frequency'},
             template='ggplot2',
             title='Rating of ChatGPT Application')

# update the plot layout
fig.update_layout(width=900, height=500)

# change the color of the bar
fig.update_traces(marker_color='rebeccapurple')

fig.show()

""" Counting Phrase Frequencies"""

phrase_counts = X.sum(axis=0)
phrases = vectorizer.get_feature_names_out()
phrase_freq = [(phrases[i], phrase_counts[0, i]) for i in range(len(phrases))]
# sort phrases by frequency
phrase_freq = sorted(phrase_freq, key=lambda x: x[1], reverse=True)

phrase_df = pd.DataFrame(phrase_freq, columns=['Phrase', 'Frequency'])

"""Plotting Common Positive Phrases"""

phrase_df = pd.DataFrame(phrase_freq, columns=['Phrase', 'Frequency'])
fig = px.bar(phrase_df,
             x='Frequency',
             y='Phrase',
             orientation='h',
             title='Top Common Phrases in Positive Reviews',
             width=1000,
             height=600)


fig.update_layout(
    xaxis_title='Frequency',
    yaxis_title='Phrase',
    yaxis={'categoryorder':'total ascending'}
)
fig.show()

"""Extracting Common Phrases in Negative Reviews"""

# filter reviews with negative sentiment
negative_reviews = df[df['Sentiment'] == 'Negative']['Review']

# use CountVectorizer to extract common phrases (n-grams) for negative reviews
X_neg = vectorizer.fit_transform(negative_reviews)

# sum the counts of each phrase in negative reviews
phrase_counts_neg = X_neg.sum(axis=0)
phrases_neg = vectorizer.get_feature_names_out()
phrase_freq_neg = [(phrases_neg[i], phrase_counts_neg[0, i]) for i in range(len(phrases_neg))]

# sort phrases by frequency
phrase_freq_neg = sorted(phrase_freq_neg, key=lambda x: x[1], reverse=True)

"""Plotting Common Negative Phrases"""

phrase_neg_df = pd.DataFrame(phrase_freq_neg, columns=['Phrase', 'Frequency'])
fig = px.bar(phrase_neg_df,
             x='Frequency',
             y='Phrase',
             orientation='h',
             title='Top Common Phrases in Negative Reviews',
             width=1000,
             height=600)

fig.update_layout(
    xaxis_title='Frequency',
    yaxis_title='Phrase',
    yaxis={'categoryorder':'total ascending'}
)
fig.show()

""" Grouping Similar Phrases into Problem Categories"""

problem_keywords = {
    'Incorrect Answers': ['wrong answer', 'gives wrong', 'incorrect', 'inaccurate', 'wrong'],
    'App Performance': ['slow', 'lag', 'crash', 'bug', 'freeze', 'loading', 'glitch', 'worst app', 'bad app', 'horrible', 'terrible'],
    'User Interface': ['interface', 'UI', 'difficult to use', 'confusing', 'layout'],
    'Features Missing/Not Working': ['feature missing', 'not working', 'missing', 'broken', 'not available'],
    'Quality of Responses': ['bad response', 'useless', 'poor quality', 'irrelevant', 'nonsense']
}

# initialize a dictionary to count problems
problem_counts = {key: 0 for key in problem_keywords.keys()}

"""Counting Problems in Negative Reviews"""

for phrase, count in phrase_freq_neg:
    for problem, keywords in problem_keywords.items():
        if any(keyword in phrase for keyword in keywords):
            problem_counts[problem] += count
            break

""" Plotting Common Problems"""

problem_df = pd.DataFrame(list(problem_counts.items()), columns=['Problem', 'Frequency'])
fig = px.bar(problem_df,
             x='Frequency',
             y='Problem',
             orientation='h',
             title='Common Problems Faced by Users in ChatGPT',
             width=1000,
             height=600)


fig.update_layout(
    plot_bgcolor='white',
    paper_bgcolor='white',
    xaxis_title='Frequency',
    yaxis_title='Problem',
    yaxis={'categoryorder':'total ascending'}
)

fig.show()

"""Sentiment Trends Over Time"""

# convert 'Review Date' to datetime format
df['Review Date'] = pd.to_datetime(df['Review Date'])

# aggregate sentiment counts by date
sentiment_over_time = df.groupby([df['Review Date'].dt.to_period('M'), 'Sentiment']).size().unstack(fill_value=0)

# convert the period back to datetime for plotting
sentiment_over_time.index = sentiment_over_time.index.to_timestamp()

"""Plotting Sentiment Trends"""

fig = go.Figure()
fig.add_trace(go.Scatter(x=sentiment_over_time.index, y=sentiment_over_time['Positive'], mode='lines', name='Positive', line=dict(color='green')))
fig.add_trace(go.Scatter(x=sentiment_over_time.index, y=sentiment_over_time['Neutral'], mode='lines', name='Neutral', line=dict(color='gray')))
fig.add_trace(go.Scatter(x=sentiment_over_time.index, y=sentiment_over_time['Negative'], mode='lines', name='Negative', line=dict(color='red')))
fig.update_layout(
    title='Sentiment Trends Over Time',
    xaxis_title='Date',
    yaxis_title='Number of Reviews',
    plot_bgcolor='white',
    legend_title_text='Sentiment'
)
fig.show()

"""Net Promoter Score (NPS) Calculation"""

# define the categories based on the ratings
df['NPS Category'] = df['Ratings'].apply(lambda x: 'Promoter' if x == 5 else ('Passive' if x == 4 else 'Detractor'))

# calculate the percentage of each category
nps_counts = df['NPS Category'].value_counts(normalize=True) * 100

# calculate NPS
nps_score = nps_counts.get('Promoter', 0) - nps_counts.get('Detractor', 0)

# display the NPS Score
nps_score